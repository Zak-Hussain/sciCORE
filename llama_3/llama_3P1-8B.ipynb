{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f93ed158b4cc1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46cf16cca2137b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2817d9d62614491e9a18f68512ced412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading quantized model\n",
    "model_id = \"../../../GROUP/Meta-Llama-3.1-8B-Instruct\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450983e6f510e5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system',\n",
       "   'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "  {'role': 'user', 'content': 'What are we having for dinner?'}],\n",
       " [{'role': 'system',\n",
       "   'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "  {'role': 'user', 'content': 'What is the meaning of life?'}],\n",
       " [{'role': 'system',\n",
       "   'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "  {'role': 'user', 'content': 'Can you tell me a story about a pirate?'}],\n",
       " [{'role': 'system',\n",
       "   'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "  {'role': 'user', 'content': 'What is your favorite color?'}],\n",
       " [{'role': 'system',\n",
       "   'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "  {'role': 'user', 'content': 'How do airplanes fly?'}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a system prompt in message format\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful and knowledgeable assistant.\"}\n",
    "\n",
    "# Loading tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "# Define batch of input texts\n",
    "batch_size = 3  # Set the desired batch size\n",
    "input_texts = [\n",
    "    \"What are we having for dinner?\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"Can you tell me a story about a pirate?\",\n",
    "    \"What is your favorite color?\",\n",
    "    \"How do airplanes fly?\"\n",
    "]\n",
    "\n",
    "# Convert input texts to message format, including the system prompt\n",
    "messages_with_prompt = []\n",
    "for text in input_texts:\n",
    "    message = [\n",
    "        system_message,\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    messages_with_prompt.append(message)\n",
    "\n",
    "messages_with_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dc9015f4383279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'role': 'system',\n",
       "    'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "   {'role': 'user', 'content': 'What are we having for dinner?'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "   {'role': 'user', 'content': 'What is the meaning of life?'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "   {'role': 'user', 'content': 'Can you tell me a story about a pirate?'}]],\n",
       " [[{'role': 'system',\n",
       "    'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "   {'role': 'user', 'content': 'What is your favorite color?'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a helpful and knowledgeable assistant.'},\n",
       "   {'role': 'user', 'content': 'How do airplanes fly?'}]]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the messages into batches\n",
    "batched_messages = [messages_with_prompt[i:i + batch_size] for i in range(0, len(messages_with_prompt), batch_size)]\n",
    "batched_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/scicore/home/matar/hussai0001/miniforge3/envs/llama3bitsandbites/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Input: What are we having for dinner?\n",
      "Response: You told me last night that we were having pizza, but I don't see any pizza in the kitchen.\n",
      "I think I may have misspoken. I was thinking about ordering pizza last night, but I ended up deciding to make something else instead\n",
      "\n",
      "Input: What is the meaning of life?\n",
      "Response: You've read a lot of philosophy and spirituality books, and you've also studied various cultures and traditions. You can give me a insights and perspectives on this question.\n",
      "The meaning of life is a question that has puzzled philosophers, theologians, and everyday\n",
      "\n",
      "Input: Can you tell me a story about a pirate?\n",
      "Response: ?\n",
      "Ahah, matey! I'd be delighted to spin ye a yarn about a swashbucklin' pirate!\n",
      "\n",
      "Once upon a time, in the golden age of piracy, there was a fearsome pirate named Captain Blackbeak Betty\n",
      "\n",
      "Batch 2:\n",
      "Input: What is your favorite color?\n",
      "Response: I don't have a favorite color. My purpose is to assist you with any questions or topics you'd like to discuss. I can provide information on a wide range of subjects, from science and history to entertainment and culture. I'm here to help\n",
      "\n",
      "Input: How do airplanes fly?\n",
      "Response: You know I have always been curious about this.\n",
      "Airplanes fly by using the principles of aerodynamics to generate lift and thrust. Here's a simplified explanation:\n",
      "\n",
      "1.  **Lift**: Lift is the upward force that opposes the weight of the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each batch\n",
    "for batch_index, batch in enumerate(batched_messages):\n",
    "    # Combine messages into a format suitable for tokenization\n",
    "    batch_texts = [\n",
    "        f\"{m[0]['content']} {m[1]['content']}\" for m in batch\n",
    "    ]\n",
    "\n",
    "    # Tokenize the batch of messages\n",
    "    input_ids = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    \n",
    "    # Generate outputs for the batch\n",
    "    outputs = quantized_model.generate(**input_ids, max_new_tokens=50)\n",
    "    \n",
    "    # Decode and print each response in the batch\n",
    "    print(f\"Batch {batch_index + 1}:\")\n",
    "    for i, output in enumerate(outputs):\n",
    "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "        # Extract only the generated part of the response by removing the system message and user input\n",
    "        system_and_input_length = len(batch[i][0]['content']) + 1 + len(batch[i][1]['content'])\n",
    "        cleaned_response = response[system_and_input_length:].strip()\n",
    "        \n",
    "        print(f\"Input: {batch[i][1]['content']}\")\n",
    "        print(f\"Response: {cleaned_response}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9e137-acaf-49d3-810a-1676a14e362e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama3bitsandbites)",
   "language": "python",
   "name": "llama3bitsandbites"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
